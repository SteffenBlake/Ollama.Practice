# Ollama.Practice

## Requirements

### (If on Windows) Windows Subsystem for Linux
Confirm via `wsl --list --verbose`
https://learn.microsoft.com/en-us/windows/wsl/install

**All steps below assume you are doing them inside a linux environment, windows is not actively supported**

### Python 3.12.x (NOT 3.13! It isnt compatible atm due to downstream stuff) 
Confirm via `python --version`
https://www.python.org/downloads/
or
https://github.com/pyenv/pyenv for better python version management

### .NET 9
Confirm via `dotnet --list-sdks`
https://dotnet.microsoft.com/en-us/download

### Docker v28+ (might work on earlier, but I only have it tested on 28+)
Confirm via `docker --version`
https://docs.docker.com/engine/install/

### Nvidia Cuda 12 supported GPU
Confirm via `nvidia-smi`
Check for `CUDA Version: 12.x` or better
https://www.nvidia.com/en-in/drivers/

### nvidia-container-toolkit
Confirm via `docker run --rm --gpus all nvidia/cuda:12.9.0-base-ubuntu22.04 nvidia-smi`
You should get the exact same output as the prior step (This runs `nvidia-smi` inside a container, confirming docker can see and use your GPU)
https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html

## Running the application
1. Checkout this repo via git: `git clone git@github.com:SteffenBlake/Ollama.Practice.git`
2. `cd` into `./src/Ollama.Practice.AppHost/`
3. `dotnet run`
4. You'll see a message in the terminal of `Login to the dashboard at https://0.0.0.0:17010/login?t=<token>`, open this url in your browser to access the Aspire Dashboard (you may need to change 0.0.0.0 to localhost)
5. Eventually the `crew` job will display `Finished`, at which point you can check out its logs, or you can navigate to `src/Ollama.Practice.Crew/output/<timestamp>/...` to see the various output text files it produced.

# Aspire Resources
* `aspire-dashboard` - This is the aspire dashboard itself, it often displays a state of unhealthy but seems to work fine anyways.

* `PdfUrl` - What URL to download the PDF for RAGing from (Configured param)
* `ModelName` - HuggingFace model name that Ollama will download and use automatically (Configured param)
* `ollama-ollama` - docker image of the Ollama server, which lets you self host an LLM with the OpenAI API spec
* `qdrant-ollama` - Qdrant Vector Database, for loading semantic search info into, and searching on
* `memory-builder` - Startup service, written in python, that downloads `PdfUrl` and chunks it, then loads it into `qdrant-ollama` vectorDB, then finishes. Will short circuit out if the collection already exists.
* `crew` - Final service that actually performs work on the data. Spins up 3 agents. Summarizer, Quiz Maker, and Quiz Taker.

# Agents

* `Summarizer` - Asynchronously generates a `summary.txt` in the output director (See above), which summarizes the info from the vectorDB
* `Quiz Maker` - Generates `quiz.txt`, a set of multiple choice questions, using info from the vector DB
* `Quiz Taker` - Answers the multiple choice questions generated by `Quiz Maker`, and outputs this as `answers.txt`, also using the vectorDB to find the answers.
